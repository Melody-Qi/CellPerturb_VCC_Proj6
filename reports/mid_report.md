# Dataset Overview

**Dataset Source:** Virtual Cell Challenge 2025  
**Input file:** `adata_Training.h5ad`  
**Format:** AnnData (single-cell standard)

**Dataset shape**
- **221,273 cells Ã— 18,080 genes**
- **150 perturbation target genes**
- **38176 non-targeting/control cells**

**Annotations (obs)**
- `target_gene`: perturbed gene  
- `guide_id`: specific gRNA  
- `batch`: sequencing batch

**Gene info (var)**
- `gene_id`: stable ID for each gene

> **Speaker notes:**  
> This study uses the perturbation-based single-cell dataset provided by the Virtual Cell Challenge.  
> It contains ~221k cells, ~18k genes and 150 perturbed (target) genes. Metadata includes gRNA, batch and perturbation labels used as supervision.

---

# Data Distribution

## 1 Severe imbalance in perturbation samples
- Some perturbations have **>10,00 cells**
- Many perturbations have **<500 cells**

## 2 Strong batch effects
- Multiple sequencing batches present
- scRNA-seq batch effects can dominate biological signal

## 3 Large variation in DEG counts across perturbations
- Different perturbations yield drastically different numbers of DEGs  
- Some targets show only a few hundred DEGs (e.g., **MED13: 204 DEGs, 70 up / 134 down**)  
- Others show thousands (e.g., **KDM1A: 3053 DEGs, 2194 up / 859 down**)  
- This creates heterogeneity in signal strength and complicates model training and evaluation

> **Speaker notes:**  
> Beyond sample imbalance and batch effects, perturbations differ dramatically in the strength of their transcriptional response â€” some produce only modest changes while others induce thousands of DEGs. This wide dynamic range makes it challenging for models to generalize across perturbations with very different effect sizes.

---

# Data Preprocessing

1. **Normalization**  
   Makes gene expression comparable across cells.  

2. **Log1p Transformation**  
   Stabilizes variance and reduces the dominance of highly expressed genes.

3. **Highly Variable Gene (HVG) Selection**  
   Identify the top **2,000 genes** with the highest normalized dispersion.

4. **Output Matrix**  
   Store processed features in `obsm["X_hvg"]` (shape: *n_cells Ã— 2000*).

> **Speaker notes:**  
> We perform standard scRNA-seq preprocessing (normalization + HVG selection), compute virtual-cell embeddings for STATE, and use ESM2 or similar embeddings for perturbations. Splits are by perturbation gene to avoid leakage.

---

# Dataset Split Strategy

## Why split by perturbation gene?
- To evaluate model **generalization to unseen perturbations** (zero-shot)

## Example split (by pert. gene)
- **Training:** 64% of perturbation genes  
- **Validation:** 16%  
- **Test:** 30-gene test set 20%  
- Controls (non-targeting) included in all splits  
- **No overlap** of perturbation genes between splits

## Benefits
- Prevents label leakage across splits
- Tests zero-shot transfer capability
- Ensures fair evaluation of perturbation prediction

> **Speaker notes:**  
> We split at the perturbation-gene level (not by cells). This avoids leakage and lets us measure whether a model can predict effects for genes it never saw during training.

---
# ğŸ§¬ STATE Model Reproduction â€” Summary

---

## 1ï¸âƒ£ Core Idea of STATE
- **SE (State Embedding):**  
  Learns biologically informed cell embeddings using gene-level ESM2 features + Transformer.
- **ST (State Transition):**  
  Learns how a perturbation transforms control â†’ perturbed cells using a Transformer + **MMD loss**.

**Goal:** Model how genetic perturbations reshape single-cell transcriptomes.

---

## 2ï¸âƒ£ Model Architecture (High-Level)

### **SE Module**
- Input: top **2048 expressed genes** per cell  
- Gene embeddings from **ESM2 â†’ 672-dim**  
- Add expression-value soft-binning (10 bins)  
- Sequence = genes + **[CLS]** + **[DS]**  
- Transformer encoder â†’ **cell embedding (682-dim)**

### **ST Module**
- Inputs:
  - control cell embedding  
  - perturbation embedding  
  - batch embedding  
- Project to shared hidden dim  
- Transformer backbone models â€œstate shiftâ€  
- Output: predicted perturbed expression  
- Loss: **MMD** between predicted vs real distributions

---
---

## ğŸŒ æ€»ä½“ç»“æ„æ¦‚è§ˆ

STATE æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š

| æ¨¡å— | åç§° | ä½œç”¨ | è®­ç»ƒæ–¹å¼ |
|------|------|------|-----------|
| ğŸ§  **SE (State Embedding)** | å­¦ä¹ å•ç»†èƒçš„è¡¨ç¤º | è‡ªç›‘ç£è®­ç»ƒï¼ˆself-supervisedï¼‰ | ç”Ÿæˆ cell embedding |
| ğŸ” **ST (State Transition)** | å­¦ä¹ æ‰°åŠ¨å¦‚ä½•æ”¹å˜ç»†èƒçŠ¶æ€ | æœ‰ç›‘ç£è®­ç»ƒï¼ˆsupervisedï¼‰ | å®ç° control â†’ perturbed è½¬æ¢ |

---

## ğŸ§© SE æ¨¡å—ï¼ˆState Embedding, Â§4.4ï¼‰

SE æ¨¡å‹ç”¨äºå­¦ä¹ æ¯ä¸ªç»†èƒçš„é«˜ç»´ embeddingã€‚å…¶æ ¸å¿ƒæ€è·¯æ˜¯ï¼š
> åˆ©ç”¨è›‹ç™½è¯­è¨€æ¨¡å‹ï¼ˆESM2ï¼‰å°†æ¯ä¸ªåŸºå› è½¬åŒ–ä¸ºè¯­ä¹‰å‘é‡ï¼Œå†é€šè¿‡ Transformer å»ºæ¨¡åŸºå› é—´å…³ç³»ï¼Œä»è€Œè·å¾—ç»†èƒçº§åˆ«çš„è¡¨å¾ã€‚

### ğŸ”¬ ç»“æ„æ­¥éª¤ä¸è®ºæ–‡å¯¹åº”

| æ­¥éª¤ | è®ºæ–‡å‡ºå¤„ | è¯´æ˜ | è¾“å‡ºç»´åº¦ |
|------|-----------|------|-----------|
| 1ï¸âƒ£ **åŸºå› å±‚ embedding** | Eq. (23) | ç”¨ **ESM2** ç”Ÿæˆæ¯ä¸ªåŸºå›  5120 ç»´ embeddingï¼Œç»çº¿æ€§å±‚å‹ç¼©è‡³ `h=672` | æ¯ä¸ªåŸºå›  â†’ 672ç»´ |
| 2ï¸âƒ£ **ç»†èƒå±‚åºåˆ—** | Eq. (24) | æ¯ä¸ªç»†èƒé€‰å– **å‰ 2048 ä¸ªé«˜è¡¨è¾¾åŸºå› **ï¼ŒåŠ ä¸Š `[CLS]` å’Œ `[DS]` ä¸¤ä¸ª tokenï¼Œå½¢æˆåºåˆ—é•¿åº¦ **2050 (L+2)** | (2050 Ã— 672) |
| 3ï¸âƒ£ **è¡¨è¾¾æƒé‡åµŒå…¥** | Eq. (25)â€“(27) | é€šè¿‡ soft binning å°†è¡¨è¾¾å€¼æ˜ å°„è‡³ 10 ä¸ª binsï¼Œå¾—åˆ°è¡¨è¾¾ embeddingï¼ŒåŠ åˆ°å¯¹åº”åŸºå›  embedding ä¸Š | ä¸æ”¹å˜ç»´åº¦ (672) |
| 4ï¸âƒ£ **Transformer ç¼–ç ** | Eq. (28)â€“(30) | ç”¨ Transformer æ•æ‰ä¸Šä¸‹æ–‡ä¾èµ–ï¼›å– `[CLS]` ä½ç½®è¾“å‡ºä½œä¸ºç»†èƒæ•´ä½“è¡¨ç¤º | `e_cls âˆˆ R^672` |
| 5ï¸âƒ£ **Dataset embedding æ‹¼æ¥** | Eq. (31) | å°† `[DS]` token çš„ embeddingï¼ˆé™ç»´ä¸º 10ï¼‰æ‹¼æ¥åˆ° `e_cls` åï¼Œå¾—åˆ°æœ€ç»ˆç»†èƒè¡¨ç¤º | `z_cell âˆˆ R^(672+10)=R^682` |

---

## ğŸ” ST æ¨¡å—ï¼ˆState Transition, Â§4.3ï¼‰

ST æ¨¡å—å­¦ä¹ å¦‚ä½•å°†â€œcontrol ç»†èƒâ€è½¬æ¢ä¸ºâ€œperturbed ç»†èƒâ€ã€‚

### âœ³ï¸ è¾“å…¥ç»„æˆ

| è¾“å…¥å˜é‡ | å«ä¹‰ | å½¢çŠ¶ | è¯´æ˜ |
|-----------|------|------|------|
| `X_ctrl` | control ç»†èƒè¡¨è¾¾çŸ©é˜µ | B Ã— S Ã— G | æ§åˆ¶ç»„ç»†èƒçš„è¡¨è¾¾ï¼ˆæˆ– embeddingï¼‰ |
| `Z_pert` | perturbation embedding | B Ã— S Ã— D_pert | æ‰°åŠ¨æ¡ä»¶ï¼ˆå¦‚åŸºå› æ•²é™¤ï¼‰ |
| `Z_batch` | batch embedding | B Ã— S Ã— D_batch | æ‰¹æ¬¡ä¿¡æ¯ï¼ˆæŠ€æœ¯å™ªå£°æ§åˆ¶ï¼‰ |

å‚æ•°å®šä¹‰ï¼š
| ç¬¦å· | å«ä¹‰ | å…¸å‹å€¼ |
|-------|--------|---------|
| G | åŸºå› æ•° | 18,080 |
| S | æ¯ä¸ª set çš„ cell æ•° |  |
| B | batch æ•°ï¼ˆmini-batch sizeï¼‰ |  |
| dh | hidden dim |  |
| D_pert | æ‰°åŠ¨ embedding ç»´åº¦ï¼ˆESM2 å‹ç¼©åï¼‰ |  |

---

### âš™ï¸ æ ¸å¿ƒè®¡ç®—æµç¨‹

1ï¸âƒ£ **åµŒå…¥å±‚**  
æ‰€æœ‰è¾“å…¥æ˜ å°„åˆ°ç›¸åŒ hidden dimï¼š
$$
H = H_{cell} + H_{pert} + H_{batch}
$$
- $ H_{cell} = f_{cell}(X_{ctrl}) $ï¼Œ ( â†’ dh)  
- $ H_{pert} = f_{pert}(Z_{pert}) $ï¼Œ ( â†’ dh)  
- $ H_{batch} = f_{batch}(Z_{batch}) $ï¼Œ ( â†’ dh)

2ï¸âƒ£ **Transformer Backbone**  
$$
O = H + f_{ST}(H)
$$
- $ f_{ST} $ï¼štransformer å±‚ï¼Œæ•æ‰ perturbation å¯¹ç»†èƒåˆ†å¸ƒçš„å½±å“ã€‚

3ï¸âƒ£ **è¾“å‡ºå±‚**  
çº¿æ€§å±‚å°† hidden è¡¨ç¤ºæ˜ å°„å›åŸºå› è¡¨è¾¾ç©ºé—´ï¼š  
$$
\hat{X}_{target} = f_{recon}(O) = O W_{recon} + b_{recon}
$$

è¾“å‡ºç»´åº¦ï¼š  
$$
\hat{X}_{target} \in \mathbb{R}^{B \times S \times G}
$$

---

### ğŸ“‰ æŸå¤±å‡½æ•°ï¼šMMDï¼ˆMaximum Mean Discrepancyï¼‰

ST æ¨¡å‹ä½¿ç”¨ MMD loss è¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸çœŸå®æ‰°åŠ¨åˆ†å¸ƒçš„å·®å¼‚
---

## 3ï¸âƒ£ Reproduction Workflow

### **Data Preparation**
- Load VCC training data  
- log-transform   

### **ST Module**
 

---


