{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fix DES based on hpdex de_result.csv\n",
    "- PDS and MAE remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from typing import Optional, Dict, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import anndata as ad\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from typing import Optional, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import anndata as ad\n",
    "\n",
    "def _get_matrix_row_mean(adata, mask):\n",
    "    \"\"\"\n",
    "    返回 mask（布尔数组)对应细胞的每个基因的平均表达（1D numpy float array)。\n",
    "    兼容稀疏矩阵。\n",
    "    \"\"\"\n",
    "    X = adata.X\n",
    "    if sp.issparse(X):\n",
    "        sub = X[mask]\n",
    "        # mean(axis=0) 返回 1xG sparse/dense matrix\n",
    "        mean_vec = np.array(sub.mean(axis=0)).ravel()\n",
    "    else:\n",
    "        sub = X[mask, :]\n",
    "        mean_vec = np.asarray(sub.mean(axis=0)).ravel()\n",
    "    return mean_vec.astype(float)\n",
    "\n",
    "def _extract_expr_vectors_for_gene(adata, mask):\n",
    "    \"\"\"返回 mask 对应细胞在所有基因上的表达子矩阵（稀疏或密集)\"\"\"\n",
    "    X = adata.X\n",
    "    if sp.issparse(X):\n",
    "        sub = X[mask].toarray()\n",
    "    else:\n",
    "        sub = X[mask, :]\n",
    "    return sub  # shape (n_cells, n_genes)\n",
    "\n",
    "def _wilcoxon_de_genes(adata, group_mask, ntc_mask, gene_names, fdr=0.05, sample_n:int=1000):\n",
    "    \"\"\"\n",
    "    对每个基因做 Wilcoxon rank-sum（Mann-Whitney U)检验：group vs ntc。\n",
    "    为控制时间，当每组细胞非常多时，会进行随机抽样（sample_n)。\n",
    "    返回：DataFrame 包含 index=gene_names, columns=['pval','logfc','mean_group','mean_ntc']\n",
    "    \"\"\"\n",
    "    n_group = int(group_mask.sum())\n",
    "    n_ntc = int(ntc_mask.sum())\n",
    "    if n_group < 2 or n_ntc < 2:\n",
    "        # 无法检验\n",
    "        return pd.DataFrame(index=gene_names, data={\n",
    "            'pval': np.ones(len(gene_names)),\n",
    "            'logfc': np.zeros(len(gene_names)),\n",
    "            'mean_group': np.zeros(len(gene_names)),\n",
    "            'mean_ntc': np.zeros(len(gene_names))\n",
    "        })\n",
    "\n",
    "    # 抽样索引\n",
    "    rng = np.random.default_rng(0)\n",
    "    if sample_n is not None:\n",
    "        if n_group > sample_n:\n",
    "            g_idx = rng.choice(np.nonzero(group_mask)[0], sample_n, replace=False)\n",
    "            group_mask_sample = np.zeros_like(group_mask, dtype=bool)\n",
    "            group_mask_sample[g_idx] = True\n",
    "        else:\n",
    "            group_mask_sample = group_mask\n",
    "        if n_ntc > sample_n:\n",
    "            n_idx = rng.choice(np.nonzero(ntc_mask)[0], sample_n, replace=False)\n",
    "            ntc_mask_sample = np.zeros_like(ntc_mask, dtype=bool)\n",
    "            ntc_mask_sample[n_idx] = True\n",
    "        else:\n",
    "            ntc_mask_sample = ntc_mask\n",
    "    else:\n",
    "        group_mask_sample = group_mask\n",
    "        ntc_mask_sample = ntc_mask\n",
    "\n",
    "    # 取表达矩阵\n",
    "    group_mat = _extract_expr_vectors_for_gene(adata, group_mask_sample)  # (n_g, G)\n",
    "    ntc_mat = _extract_expr_vectors_for_gene(adata, ntc_mask_sample)      # (n_n, G)\n",
    "\n",
    "    G = group_mat.shape[1]\n",
    "    pvals = np.ones(G)\n",
    "    mean_group = group_mat.mean(axis=0)\n",
    "    mean_ntc = ntc_mat.mean(axis=0)\n",
    "    # logFC 使用 log2(mean_group + 1) - log2(mean_ntc + 1)\n",
    "    logfc = np.log2(mean_group + 1) - np.log2(mean_ntc + 1)\n",
    "\n",
    "    # 对每个基因做 Mann-Whitney U（两侧)\n",
    "    for gi in range(G):\n",
    "        try:\n",
    "            u = mannwhitneyu(group_mat[:, gi], ntc_mat[:, gi], alternative='two-sided')\n",
    "            pvals[gi] = u.pvalue if u.pvalue is not None else 1.0\n",
    "        except Exception:\n",
    "            pvals[gi] = 1.0\n",
    "\n",
    "    # FDR 校正\n",
    "    reject, pvals_adj, _, _ = multipletests(pvals, alpha=fdr, method='fdr_bh')\n",
    "    df = pd.DataFrame({\n",
    "        'pval': pvals,\n",
    "        'pval_adj': pvals_adj,\n",
    "        'significant': reject,\n",
    "        'logfc': logfc,\n",
    "        'mean_group': mean_group,\n",
    "        'mean_ntc': mean_ntc\n",
    "    }, index=gene_names)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _load_de_table(df_or_path: Optional[Union[str, pd.DataFrame]]):\n",
    "    \"\"\"\n",
    "    读取或标准化一个 DE 结果表（来自 CSV 或 DataFrame）。\n",
    "    返回 standardized DataFrame with columns: ['target','feature','fdr','log2fc']\n",
    "    若参数为 None，返回 None。\n",
    "    \"\"\"\n",
    "    if df_or_path is None:\n",
    "        return None\n",
    "    # 如果是文件路径，读取\n",
    "    if isinstance(df_or_path, str):\n",
    "        df = pd.read_csv(df_or_path)\n",
    "    elif isinstance(df_or_path, pd.DataFrame):\n",
    "        df = df_or_path.copy()\n",
    "    else:\n",
    "        raise ValueError(\"true_de_df / pred_de_df must be a file path or a pandas.DataFrame (or None).\")\n",
    "\n",
    "    # 统一小写列名便于匹配\n",
    "    colmap = {c.lower(): c for c in df.columns}\n",
    "    # identify columns for fdr and log2fc\n",
    "    fdr_col = None\n",
    "    for cand in ['fdr', 'pval_adj', 'padj', 'adj_pval', 'adj_pvalue']:\n",
    "        if cand in colmap:\n",
    "            fdr_col = colmap[cand]\n",
    "            break\n",
    "    if fdr_col is None:\n",
    "        # fallback: use p_value and then will BH adjust later (less ideal but still)\n",
    "        if 'p_value' in colmap or 'pval' in colmap or 'p_value' in [c.lower() for c in df.columns]:\n",
    "            # leave fdr_col None -> caller might choose to compute BH, but here we set fdr to pval to be safe\n",
    "            pass\n",
    "\n",
    "    logfc_col = None\n",
    "    for cand in ['log2_fold_change', 'log2fc', 'logfc', 'log2_foldchange']:\n",
    "        if cand in colmap:\n",
    "            logfc_col = colmap[cand]\n",
    "            break\n",
    "\n",
    "    target_col = None\n",
    "    for cand in ['target', 'target_gene', 'perturbation', 'group']:\n",
    "        if cand in colmap:\n",
    "            target_col = colmap[cand]\n",
    "            break\n",
    "    if target_col is None:\n",
    "        raise ValueError(\"DE table has no target column (expected 'target'/'target_gene' etc.).\")\n",
    "\n",
    "    feature_col = None\n",
    "    for cand in ['feature', 'gene', 'gene_name', 'geneid']:\n",
    "        if cand in colmap:\n",
    "            feature_col = colmap[cand]\n",
    "            break\n",
    "    if feature_col is None:\n",
    "        raise ValueError(\"DE table has no feature/gene column (expected 'feature'/'gene' etc.).\")\n",
    "\n",
    "    # build standardized df\n",
    "    std = pd.DataFrame()\n",
    "    std['target'] = df[target_col].astype(str)\n",
    "    std['feature'] = df[feature_col].astype(str)\n",
    "\n",
    "    # fdr: if available use it; else try to use pval and BH adjust\n",
    "    if fdr_col is not None:\n",
    "        std['fdr'] = pd.to_numeric(df[fdr_col], errors='coerce').fillna(1.0)\n",
    "    else:\n",
    "        # try pval column\n",
    "        pcol = None\n",
    "        for cand in ['p_value', 'pval', 'p.value', 'pvalue', 'p_value']:\n",
    "            if cand in colmap:\n",
    "                pcol = colmap[cand]\n",
    "                break\n",
    "        if pcol is not None:\n",
    "            pvals = pd.to_numeric(df[pcol], errors='coerce').fillna(1.0).values\n",
    "            # perform BH per entire table (conservative but acceptable); note we don't know grouping yet\n",
    "            _, p_adj, _, _ = multipletests(pvals, method='fdr_bh')\n",
    "            std['fdr'] = p_adj\n",
    "        else:\n",
    "            std['fdr'] = 1.0  # fallback: no significance\n",
    "\n",
    "    # log2fc: if absent, try to compute from fold_change column if present\n",
    "    if logfc_col is not None:\n",
    "        std['log2fc'] = pd.to_numeric(df[logfc_col], errors='coerce').fillna(0.0)\n",
    "    else:\n",
    "        # try fold_change\n",
    "        fc_col = None\n",
    "        for cand in ['fold_change', 'foldchange', 'fc']:\n",
    "            if cand in colmap:\n",
    "                fc_col = colmap[cand]\n",
    "                break\n",
    "        if fc_col is not None:\n",
    "            fc_vals = pd.to_numeric(df[fc_col], errors='coerce').fillna(1.0)\n",
    "            # avoid log of nonpositive\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                log2fc = np.log2(fc_vals.replace(0, np.nan)).fillna(0.0)\n",
    "            std['log2fc'] = log2fc\n",
    "        else:\n",
    "            std['log2fc'] = 0.0\n",
    "\n",
    "    return std[['target', 'feature', 'fdr', 'log2fc']]\n",
    "\n",
    "def compute_vcc_scores(true_adata,\n",
    "                       pred_adata,\n",
    "                       baseline_scores: Optional[Dict[str,float]] = None,\n",
    "                       baseline_adata = None,\n",
    "                       groupby: str = 'target_gene',\n",
    "                       control_label: str = 'non-targeting',\n",
    "                       fdr: float = 0.05,\n",
    "                       sample_n: int = 1000,\n",
    "                       min_cells_per_group: int = 3,\n",
    "                       true_de_df: Optional[Union[str,pd.DataFrame]] = None,\n",
    "                       pred_de_df: Optional[Union[str,pd.DataFrame]] = None):\n",
    "    \"\"\"\n",
    "    计算 DES, PDS, MAE 以及最终 S。\n",
    "    新增参数：\n",
    "        true_de_df / pred_de_df: 可选路径或 DataFrame，表示 Wilcoxon 的结果 CSV（columns include target, feature, fdr, log2_fold_change 等）。\n",
    "                                 若提供，则 DES 的 G_true / G_pred 直接从这些表中读取（使用 fdr阈值），而不再对 AnnData 做 Wilcoxon 计算。\n",
    "    其余行为与原函数一致（PDS/MAE 没变）。\n",
    "    \"\"\"\n",
    "    # --- 对齐基因（var_names) ---\n",
    "    genes_true = list(true_adata.var_names)\n",
    "    genes_pred = list(pred_adata.var_names)\n",
    "    if genes_true != genes_pred:\n",
    "        # 取交集并重新索引\n",
    "        common = [g for g in genes_true if g in genes_pred]\n",
    "        if len(common) == 0:\n",
    "            raise ValueError(\"No common genes between true_adata and pred_adata.\")\n",
    "        true_adata = true_adata[:, common]\n",
    "        pred_adata = pred_adata[:, common]\n",
    "\n",
    "    gene_names = list(true_adata.var_names)\n",
    "    G = len(gene_names)\n",
    "\n",
    "    # --- DE CSV (若有) 读取并标准化 ---\n",
    "    true_de_table = _load_de_table(true_de_df)  # may be None\n",
    "    pred_de_table = _load_de_table(pred_de_df)  # may be None\n",
    "\n",
    "    # 如果提供了 DE 表，但其中基因/perturbation名与 adata 不匹配，发出警告并用交集\n",
    "    if true_de_table is not None:\n",
    "        # only keep features that are in gene_names\n",
    "        before = true_de_table.shape[0]\n",
    "        true_de_table = true_de_table[true_de_table['feature'].isin(gene_names)].copy()\n",
    "        after = true_de_table.shape[0]\n",
    "        if after < before:\n",
    "            warnings.warn(\"Some genes in provided true_de_df not found in AnnData var_names; they were dropped.\")\n",
    "    if pred_de_table is not None:\n",
    "        before = pred_de_table.shape[0]\n",
    "        pred_de_table = pred_de_table[pred_de_table['feature'].isin(gene_names)].copy()\n",
    "        after = pred_de_table.shape[0]\n",
    "        if after < before:\n",
    "            warnings.warn(\"Some genes in provided pred_de_df not found in AnnData var_names; they were dropped.\")\n",
    "\n",
    "    # --- 确定 perturbation 列与分组 ---\n",
    "    if groupby not in true_adata.obs.columns:\n",
    "        raise ValueError(f\"groupby '{groupby}' not found in true_adata.obs\")\n",
    "    if groupby not in pred_adata.obs.columns:\n",
    "        raise ValueError(f\"groupby '{groupby}' not found in pred_adata.obs\")\n",
    "\n",
    "    true_groups = true_adata.obs[groupby].astype(str)\n",
    "    pred_groups = pred_adata.obs[groupby].astype(str)\n",
    "\n",
    "    # 找到控制(ntc) mask\n",
    "    ntc_mask_true = (true_groups == control_label).values\n",
    "    ntc_mask_pred = (pred_groups == control_label).values\n",
    "\n",
    "    if ntc_mask_true.sum() < min_cells_per_group or ntc_mask_pred.sum() < min_cells_per_group:\n",
    "        warnings.warn(\"Control (ntc) cell count is small.\")\n",
    "\n",
    "    # 获取 perturbation 列表（排除 control）\n",
    "    perturbations = sorted([g for g in true_groups.unique() if g != control_label])\n",
    "    N = len(perturbations)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"No perturbations found (after excluding control_label).\")\n",
    "\n",
    "    # --- 1) 计算 DES per perturbation ---\n",
    "    DES_list = []\n",
    "    for p in perturbations:\n",
    "        # masks\n",
    "        g_mask_true = (true_groups == p).values\n",
    "        g_mask_pred = (pred_groups == p).values\n",
    "\n",
    "        if g_mask_true.sum() < min_cells_per_group or ntc_mask_true.sum() < min_cells_per_group:\n",
    "            DES_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # 1a) G_true: 如果提供了 true_de_table，则直接从表中取 G_true；否则运行 Wilcoxon\n",
    "        if true_de_table is not None:\n",
    "            sub = true_de_table[true_de_table['target'] == p]\n",
    "            G_true = set(sub.loc[sub['fdr'] <= fdr, 'feature'].tolist())\n",
    "        else:\n",
    "            df_true = _wilcoxon_de_genes(true_adata, g_mask_true, ntc_mask_true, gene_names, fdr=fdr, sample_n=sample_n)\n",
    "            G_true = set(df_true.index[df_true['significant']].tolist())\n",
    "\n",
    "        # 1b) G_pred: 如果提供了 pred_de_table，则直接从表中取 G_pred；否则运行 Wilcoxon on pred_adata\n",
    "        if pred_de_table is not None:\n",
    "            subp = pred_de_table[pred_de_table['target'] == p]\n",
    "            G_pred = set(subp.loc[subp['fdr'] <= fdr, 'feature'].tolist())\n",
    "        else:\n",
    "            if g_mask_pred.sum() < min_cells_per_group or ntc_mask_pred.sum() < min_cells_per_group:\n",
    "                df_pred = pd.DataFrame(index=gene_names, data={\n",
    "                    'pval': np.ones(G),\n",
    "                    'pval_adj': np.ones(G),\n",
    "                    'significant': np.zeros(G, dtype=bool),\n",
    "                    'logfc': np.zeros(G),\n",
    "                    'mean_group': np.zeros(G),\n",
    "                    'mean_ntc': np.zeros(G)\n",
    "                })\n",
    "            else:\n",
    "                df_pred = _wilcoxon_de_genes(pred_adata, g_mask_pred, ntc_mask_pred, gene_names, fdr=fdr, sample_n=sample_n)\n",
    "            G_pred = set(df_pred.index[df_pred['significant']].tolist())\n",
    "\n",
    "        # 若 |G_true| == 0：按原逻辑跳过（记 NaN）\n",
    "        if len(G_true) == 0:\n",
    "            DES_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Case 分支：|G_pred| <= |G_true| 或 > \n",
    "        if len(G_pred) <= len(G_true):\n",
    "            inter = len(G_pred & G_true)\n",
    "            DES_k = inter / len(G_true)\n",
    "            DES_list.append(DES_k)\n",
    "        else:\n",
    "            # > 情况：需要按预测的绝对 log2FC 排序取 top |G_true|\n",
    "            # 如果 pred_de_table 提供了 log2fc，则使用它；若没有，则尽量使用 df_pred 中计算结果\n",
    "            if pred_de_table is not None:\n",
    "                subp_all = pred_de_table[pred_de_table['target'] == p].set_index('feature')\n",
    "                # 若某些基因在该 perturbation 的表中缺失，补 0\n",
    "                # 这里确保索引覆盖 gene_names\n",
    "                # 取 abs(log2fc)\n",
    "                subp_all = subp_all.reindex(gene_names).fillna({'log2fc': 0.0})\n",
    "                subp_all['abs_log2fc'] = subp_all['log2fc'].abs()\n",
    "                topk = int(len(G_true))\n",
    "                top_genes = set(subp_all.sort_values('abs_log2fc', ascending=False).head(topk).index.tolist())\n",
    "            else:\n",
    "                # df_pred 已在上面构造\n",
    "                df_pred_sorted = df_pred.reindex(gene_names).copy()\n",
    "                df_pred_sorted['abs_logfc'] = np.abs(df_pred_sorted['logfc'])\n",
    "                topk = int(len(G_true))\n",
    "                top_genes = set(df_pred_sorted.sort_values('abs_logfc', ascending=False).head(topk).index.tolist())\n",
    "\n",
    "            inter = len(top_genes & G_true)\n",
    "            DES_k = inter / len(G_true)\n",
    "            DES_list.append(DES_k)\n",
    "\n",
    "    DES_array = np.array(DES_list, dtype=float)\n",
    "    DES_mean = np.nanmean(DES_array)  # 忽略 NaN\n",
    "\n",
    "    # --- 2) 计算 PDS ---\n",
    "    # 计算所有 perturbation 的 pseudobulk mean expression (true & pred), 包括 ntc\n",
    "    true_pseudobulk = {}\n",
    "    pred_pseudobulk = {}\n",
    "    for grp in list(true_groups.unique()):\n",
    "        mask = (true_groups == grp).values\n",
    "        true_pseudobulk[grp] = _get_matrix_row_mean(true_adata, mask)\n",
    "    for grp in list(pred_groups.unique()):\n",
    "        mask = (pred_groups == grp).values\n",
    "        pred_pseudobulk[grp] = _get_matrix_row_mean(pred_adata, mask)\n",
    "\n",
    "    for p in perturbations:\n",
    "        if p not in pred_pseudobulk:\n",
    "            warnings.warn(f\"Perturbation {p} missing in predicted data; using zeros vector.\")\n",
    "            pred_pseudobulk[p] = np.zeros(G)\n",
    "        if p not in true_pseudobulk:\n",
    "            warnings.warn(f\"Perturbation {p} missing in true data; using zeros vector.\")\n",
    "            true_pseudobulk[p] = np.zeros(G)\n",
    "\n",
    "    if control_label not in true_pseudobulk or control_label not in pred_pseudobulk:\n",
    "        raise ValueError(\"Control label not present in pseudobulk results for true or pred.\")\n",
    "\n",
    "    true_delta = {p: true_pseudobulk[p] - true_pseudobulk[control_label] for p in perturbations}\n",
    "    pred_delta = {p: pred_pseudobulk[p] - pred_pseudobulk[control_label] for p in perturbations}\n",
    "\n",
    "    PDS_vals = []\n",
    "    for p in perturbations:\n",
    "        dists = np.array([np.sum(np.abs(pred_delta[p] - true_delta[q])) for q in perturbations])\n",
    "        d_pt = np.sum(np.abs(pred_delta[p] - true_delta[p]))\n",
    "        rank = int((dists < d_pt).sum()) + 1\n",
    "        PDS_p = 1.0 - (rank - 1) / float(N)\n",
    "        PDS_vals.append(PDS_p)\n",
    "\n",
    "    PDS_array = np.array(PDS_vals, dtype=float)\n",
    "    PDS_mean = PDS_array.mean()\n",
    "\n",
    "    # --- 3) 计算 MAE ---\n",
    "    MAE_vals = []\n",
    "    for p in perturbations:\n",
    "        y_true = true_pseudobulk[p]\n",
    "        y_pred = pred_pseudobulk.get(p, np.zeros(G))\n",
    "        mae_k = np.mean(np.abs(y_pred - y_true))\n",
    "        MAE_vals.append(mae_k)\n",
    "    MAE_array = np.array(MAE_vals, dtype=float)\n",
    "    MAE_mean = MAE_array.mean()\n",
    "\n",
    "    # --- baseline handling ---\n",
    "    if baseline_scores is None and baseline_adata is not None:\n",
    "        baseline_res = compute_vcc_scores(true_adata, baseline_adata,\n",
    "                                          baseline_scores={'DES':0.0,'PDS':0.0,'MAE':1.0},\n",
    "                                          groupby=groupby, control_label=control_label,\n",
    "                                          fdr=fdr, sample_n=sample_n, min_cells_per_group=min_cells_per_group)\n",
    "        baseline_scores = {'DES': baseline_res['DES'],\n",
    "                           'PDS': baseline_res['PDS'],\n",
    "                           'MAE': baseline_res['MAE']}\n",
    "    if baseline_scores is None:\n",
    "        raise ValueError(\"You must provide either baseline_scores or baseline_adata to compute scaled scores.\")\n",
    "\n",
    "    DES_baseline = float(baseline_scores.get('DES', 0.0))\n",
    "    PDS_baseline = float(baseline_scores.get('PDS', 0.0))\n",
    "    MAE_baseline = float(baseline_scores.get('MAE', 1.0))\n",
    "\n",
    "    def safe_scale_score(pred, base):\n",
    "        if base >= 1.0 - 1e-12:\n",
    "            return 0.0\n",
    "        return (pred - base) / (1.0 - base)\n",
    "\n",
    "    DES_scaled = max(0,safe_scale_score(DES_mean, DES_baseline))\n",
    "    PDS_scaled = max(0,safe_scale_score(PDS_mean, PDS_baseline))\n",
    "    MAE_scaled = max(0,(MAE_baseline - MAE_mean) / MAE_baseline)\n",
    "\n",
    "    overall_score = (DES_scaled + PDS_scaled + MAE_scaled) / 3.0 * 100.0\n",
    "\n",
    "    result = {\n",
    "        'DES': float(DES_mean),\n",
    "        'PDS': float(PDS_mean),\n",
    "        'MAE': float(MAE_mean),\n",
    "        'DES_scaled': float(DES_scaled),\n",
    "        'PDS_scaled': float(PDS_scaled),\n",
    "        'MAE_scaled': float(MAE_scaled),\n",
    "        'overall_score_percent': float(overall_score),\n",
    "        'DES_per_perturbation': pd.Series(DES_array, index=perturbations),\n",
    "        'PDS_per_perturbation': pd.Series(PDS_array, index=perturbations),\n",
    "        'MAE_per_perturbation': pd.Series(MAE_array, index=perturbations),\n",
    "        'perturbations': perturbations\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def main_eval(true_file, pred_file, output_path, true_de_csv: Optional[str]=None, pred_de_csv: Optional[str]=None):\n",
    "    print(\"Loading AnnData files...\")\n",
    "    true_adata = ad.read_h5ad(true_file)\n",
    "    pred_adata = ad.read_h5ad(pred_file)\n",
    "\n",
    "    baseline_scores = {'DES': 0.106, 'PDS': 0.514, 'MAE': 0.027}\n",
    "\n",
    "    print(\"Computing scores (this may take time depending on dataset size)...\")\n",
    "    # 将 true_de_csv / pred_de_csv 传入 compute_vcc_scores（可以为 None）\n",
    "    scores = compute_vcc_scores(true_adata, pred_adata,\n",
    "                                baseline_scores=baseline_scores,\n",
    "                                groupby='target_gene',\n",
    "                                control_label='non-targeting',\n",
    "                                fdr=0.05,\n",
    "                                sample_n=1000,\n",
    "                                min_cells_per_group=3,\n",
    "                                true_de_df=true_de_csv,\n",
    "                                pred_de_df=pred_de_csv)\n",
    "\n",
    "    # Print summary and save unchanged（与原逻辑一致）\n",
    "    print(\"\\n===== Summary =====\")\n",
    "    print(f\"DES (mean): {scores['DES']:.6f}\")\n",
    "    print(f\"PDS (mean): {scores['PDS']:.6f}\")\n",
    "    print(f\"MAE (mean): {scores['MAE']:.6f}\")\n",
    "    print(f\"DES_scaled: {scores['DES_scaled']:.6f}\")\n",
    "    print(f\"PDS_scaled: {scores['PDS_scaled']:.6f}\")\n",
    "    print(f\"MAE_scaled: {scores['MAE_scaled']:.6f}\")\n",
    "    print(f\"Overall score (percent): {scores['overall_score_percent']:.4f}%\")\n",
    "\n",
    "    out_json = {\n",
    "        'DES': scores['DES'],\n",
    "        'PDS': scores['PDS'],\n",
    "        'MAE': scores['MAE'],\n",
    "        'DES_scaled': scores['DES_scaled'],\n",
    "        'PDS_scaled': scores['PDS_scaled'],\n",
    "        'MAE_scaled': scores['MAE_scaled'],\n",
    "        'overall_score_percent': scores['overall_score_percent']\n",
    "    }\n",
    "    with open(f'{output_path}.json', 'w') as f:\n",
    "        json.dump(out_json, f, indent=2)\n",
    "\n",
    "    df_des = scores['DES_per_perturbation'].rename(\"DES\").to_frame()\n",
    "    df_pds = scores['PDS_per_perturbation'].rename(\"PDS\").to_frame()\n",
    "    df_mae = scores['MAE_per_perturbation'].rename(\"MAE\").to_frame()\n",
    "    df_all = pd.concat([df_des, df_pds, df_mae], axis=1)\n",
    "    df_all.to_csv(f'{output_path}.csv')\n",
    "    print(f\"\\nSaved {output_path}.json and {output_path}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test\n",
    "\n",
    "the original DES(mean) = 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AnnData files...\n",
      "Computing scores (this may take time depending on dataset size)...\n",
      "\n",
      "===== Summary =====\n",
      "DES (mean): 0.110746\n",
      "PDS (mean): 0.600000\n",
      "MAE (mean): 2.899716\n",
      "DES_scaled: 0.005309\n",
      "PDS_scaled: 0.176955\n",
      "MAE_scaled: 0.000000\n",
      "Overall score (percent): 6.0755%\n",
      "\n",
      "Saved eval_outcome/small_set.json and eval_outcome/small_set.csv\n"
     ]
    }
   ],
   "source": [
    "main_eval(\"../small_set.h5ad\",\"../../linear_model/pred/small_set_pred.h5ad\",\"eval_outcome/small_set\",\n",
    "          \"../../evaluation/hpdex/test/small_de_results.csv\",\n",
    "          \"../../evaluation/hpdex/test/small_pred_de_results.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AnnData files...\n",
      "Computing scores (this may take time depending on dataset size)...\n",
      "\n",
      "===== Summary =====\n",
      "DES (mean): 0.093277\n",
      "PDS (mean): 0.511111\n",
      "MAE (mean): 2.904040\n",
      "DES_scaled: 0.000000\n",
      "PDS_scaled: 0.000000\n",
      "MAE_scaled: 0.000000\n",
      "Overall score (percent): 0.0000%\n",
      "\n",
      "Saved eval_outcome/valid_1119.json and eval_outcome/valid_1119.csv\n"
     ]
    }
   ],
   "source": [
    "main_eval(\"../validation_set_1119.h5ad\",\n",
    "          \"../../linear_model/pred/valid_pred_lr.h5ad\",\n",
    "          \"eval_outcome/valid_1119\",\n",
    "          \"../../evaluation/hpdex/test/valid_de_results.csv\",\n",
    "          \"../../evaluation/hpdex/test/valid_pred_de_results.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AnnData files...\n",
      "Computing scores (this may take time depending on dataset size)...\n",
      "\n",
      "===== Summary =====\n",
      "DES (mean): 0.178283\n",
      "PDS (mean): 0.519822\n",
      "MAE (mean): 2.892237\n",
      "DES_scaled: 0.080854\n",
      "PDS_scaled: 0.011980\n",
      "MAE_scaled: 0.000000\n",
      "Overall score (percent): 3.0944%\n",
      "\n",
      "Saved eval_outcome/training_1119.json and eval_outcome/training_1119.csv\n"
     ]
    }
   ],
   "source": [
    "main_eval(\"../training_set_1119.h5ad\",\n",
    "          \"../../linear_model/pred/training_lr.h5ad\",\n",
    "          \"eval_outcome/training_1119\",\n",
    "          \"../../evaluation/hpdex/test/train_de_results.csv\",\n",
    "          \"../../evaluation/hpdex/test/train_pred_de_results.csv\",)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "state1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
