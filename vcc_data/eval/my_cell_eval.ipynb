{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8377b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "计算 DES, PDS, MAE 以及最终综合得分 S 的函数集合。\n",
    "\n",
    "依赖：\n",
    "    pip install anndata scipy statsmodels numpy pandas\n",
    "\n",
    "用法（示例）：\n",
    "    scores = compute_vcc_scores(true_adata, pred_adata,\n",
    "                                groupby='target_gene', control_label='non-targeting',\n",
    "                                baseline_scores={'DES':0.2,'PDS':0.3,'MAE':0.5})\n",
    "    print(scores)\n",
    "\"\"\"\n",
    "import json\n",
    "import warnings\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import anndata as ad\n",
    "\n",
    "def _get_matrix_row_mean(adata, mask):\n",
    "    \"\"\"\n",
    "    返回 mask（布尔数组）对应细胞的每个基因的平均表达（1D numpy float array）。\n",
    "    兼容稀疏矩阵。\n",
    "    \"\"\"\n",
    "    X = adata.X\n",
    "    if sp.issparse(X):\n",
    "        sub = X[mask]\n",
    "        # mean(axis=0) 返回 1xG sparse/dense matrix\n",
    "        mean_vec = np.array(sub.mean(axis=0)).ravel()\n",
    "    else:\n",
    "        sub = X[mask, :]\n",
    "        mean_vec = np.asarray(sub.mean(axis=0)).ravel()\n",
    "    return mean_vec.astype(float)\n",
    "\n",
    "def _extract_expr_vectors_for_gene(adata, mask):\n",
    "    \"\"\"返回 mask 对应细胞在所有基因上的表达子矩阵（稀疏或密集）\"\"\"\n",
    "    X = adata.X\n",
    "    if sp.issparse(X):\n",
    "        sub = X[mask].toarray()\n",
    "    else:\n",
    "        sub = X[mask, :]\n",
    "    return sub  # shape (n_cells, n_genes)\n",
    "\n",
    "def _wilcoxon_de_genes(adata, group_mask, ntc_mask, gene_names, fdr=0.05, sample_n:int=1000):\n",
    "    \"\"\"\n",
    "    对每个基因做 Wilcoxon rank-sum（Mann-Whitney U）检验：group vs ntc。\n",
    "    为控制时间，当每组细胞非常多时，会进行随机抽样（sample_n）。\n",
    "    返回：DataFrame 包含 index=gene_names, columns=['pval','logfc','mean_group','mean_ntc']\n",
    "    \"\"\"\n",
    "    n_group = int(group_mask.sum())\n",
    "    n_ntc = int(ntc_mask.sum())\n",
    "    if n_group < 2 or n_ntc < 2:\n",
    "        # 无法检验\n",
    "        return pd.DataFrame(index=gene_names, data={\n",
    "            'pval': np.ones(len(gene_names)),\n",
    "            'logfc': np.zeros(len(gene_names)),\n",
    "            'mean_group': np.zeros(len(gene_names)),\n",
    "            'mean_ntc': np.zeros(len(gene_names))\n",
    "        })\n",
    "\n",
    "    # 抽样索引\n",
    "    rng = np.random.default_rng(0)\n",
    "    if sample_n is not None:\n",
    "        if n_group > sample_n:\n",
    "            g_idx = rng.choice(np.nonzero(group_mask)[0], sample_n, replace=False)\n",
    "            group_mask_sample = np.zeros_like(group_mask, dtype=bool)\n",
    "            group_mask_sample[g_idx] = True\n",
    "        else:\n",
    "            group_mask_sample = group_mask\n",
    "        if n_ntc > sample_n:\n",
    "            n_idx = rng.choice(np.nonzero(ntc_mask)[0], sample_n, replace=False)\n",
    "            ntc_mask_sample = np.zeros_like(ntc_mask, dtype=bool)\n",
    "            ntc_mask_sample[n_idx] = True\n",
    "        else:\n",
    "            ntc_mask_sample = ntc_mask\n",
    "    else:\n",
    "        group_mask_sample = group_mask\n",
    "        ntc_mask_sample = ntc_mask\n",
    "\n",
    "    # 取表达矩阵\n",
    "    group_mat = _extract_expr_vectors_for_gene(adata, group_mask_sample)  # (n_g, G)\n",
    "    ntc_mat = _extract_expr_vectors_for_gene(adata, ntc_mask_sample)      # (n_n, G)\n",
    "\n",
    "    G = group_mat.shape[1]\n",
    "    pvals = np.ones(G)\n",
    "    mean_group = group_mat.mean(axis=0)\n",
    "    mean_ntc = ntc_mat.mean(axis=0)\n",
    "    # logFC 使用 log2(mean_group + 1) - log2(mean_ntc + 1)\n",
    "    logfc = np.log2(mean_group + 1) - np.log2(mean_ntc + 1)\n",
    "\n",
    "    # 对每个基因做 Mann-Whitney U（两侧）\n",
    "    for gi in range(G):\n",
    "        try:\n",
    "            u = mannwhitneyu(group_mat[:, gi], ntc_mat[:, gi], alternative='two-sided')\n",
    "            pvals[gi] = u.pvalue if u.pvalue is not None else 1.0\n",
    "        except Exception:\n",
    "            pvals[gi] = 1.0\n",
    "\n",
    "    # FDR 校正\n",
    "    reject, pvals_adj, _, _ = multipletests(pvals, alpha=fdr, method='fdr_bh')\n",
    "    df = pd.DataFrame({\n",
    "        'pval': pvals,\n",
    "        'pval_adj': pvals_adj,\n",
    "        'significant': reject,\n",
    "        'logfc': logfc,\n",
    "        'mean_group': mean_group,\n",
    "        'mean_ntc': mean_ntc\n",
    "    }, index=gene_names)\n",
    "    return df\n",
    "\n",
    "def compute_vcc_scores(true_adata,\n",
    "                       pred_adata,\n",
    "                       baseline_scores: Optional[Dict[str,float]] = None,\n",
    "                       baseline_adata = None,\n",
    "                       groupby: str = 'target_gene',\n",
    "                       control_label: str = 'non-targeting',\n",
    "                       fdr: float = 0.05,\n",
    "                       sample_n: int = 1000,\n",
    "                       min_cells_per_group: int = 3):\n",
    "    \"\"\"\n",
    "    计算 DES, PDS, MAE 以及最终 S。\n",
    "    参数：\n",
    "        true_adata, pred_adata: AnnData，必须包含相同的 var（基因）或能被对齐。\n",
    "        baseline_scores: 可选字典 {'DES':float,'PDS':float,'MAE':float} 直接给出基线分数用于缩放。\n",
    "        baseline_adata: 可选，如果提供则用它与 true_adata 计算 baseline 的三个原始指标。\n",
    "                       若同时提供 baseline_scores 则以 baseline_scores 为准。\n",
    "        groupby: 在 adata.obs 中按哪个列分组（通常 'target_gene' 或 'guide_id'）\n",
    "        control_label: 对应空白/对照分组（例如 'non-targeting'）\n",
    "        fdr: Benjamini-Hochberg FDR 阈值\n",
    "        sample_n: 用于 Wilcoxon 的每组最大抽样细胞数（以控制计算量），None 表示不抽样\n",
    "        min_cells_per_group: 若某 perturbation 或 ntc 细胞数少于此阈值则跳过\n",
    "    返回：\n",
    "        dict 包含 DES, PDS, MAE, scaled 各分量，以及最终 S（百分数）\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 对齐基因（var_names） ---\n",
    "    genes_true = list(true_adata.var_names)\n",
    "    genes_pred = list(pred_adata.var_names)\n",
    "    if genes_true != genes_pred:\n",
    "        # 取交集并重新索引\n",
    "        common = [g for g in genes_true if g in genes_pred]\n",
    "        if len(common) == 0:\n",
    "            raise ValueError(\"No common genes between true_adata and pred_adata.\")\n",
    "        true_adata = true_adata[:, common]\n",
    "        pred_adata = pred_adata[:, common]\n",
    "\n",
    "    gene_names = list(true_adata.var_names)\n",
    "    G = len(gene_names)\n",
    "\n",
    "    # --- 确定 perturbation 列与分组 ---\n",
    "    if groupby not in true_adata.obs.columns:\n",
    "        raise ValueError(f\"groupby '{groupby}' not found in true_adata.obs\")\n",
    "    if groupby not in pred_adata.obs.columns:\n",
    "        raise ValueError(f\"groupby '{groupby}' not found in pred_adata.obs\")\n",
    "\n",
    "    true_groups = true_adata.obs[groupby].astype(str)\n",
    "    pred_groups = pred_adata.obs[groupby].astype(str)\n",
    "\n",
    "    # 找到控制(ntc) mask\n",
    "    ntc_mask_true = (true_groups == control_label).values\n",
    "    ntc_mask_pred = (pred_groups == control_label).values\n",
    "\n",
    "    if ntc_mask_true.sum() < min_cells_per_group or ntc_mask_pred.sum() < min_cells_per_group:\n",
    "        warnings.warn(\"Control (ntc) cell count is small.\")\n",
    "\n",
    "    # 获取 perturbation 列表（排除 control）\n",
    "    perturbations = sorted([g for g in true_groups.unique() if g != control_label])\n",
    "    N = len(perturbations)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"No perturbations found (after excluding control_label).\")\n",
    "\n",
    "    # --- 1) 计算 DES per perturbation ---\n",
    "    DES_list = []\n",
    "    for p in perturbations:\n",
    "        # masks\n",
    "        g_mask_true = (true_groups == p).values\n",
    "        g_mask_pred = (pred_groups == p).values\n",
    "\n",
    "        if g_mask_true.sum() < min_cells_per_group or ntc_mask_true.sum() < min_cells_per_group:\n",
    "            # 无法做 DE 检验，跳过（返回 NaN，这里我们不算入均值）\n",
    "            DES_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # true DE\n",
    "        df_true = _wilcoxon_de_genes(true_adata, g_mask_true, ntc_mask_true, gene_names, fdr=fdr, sample_n=sample_n)\n",
    "        G_true = set(df_true.index[df_true['significant']].tolist())\n",
    "\n",
    "        # pred DE\n",
    "        if g_mask_pred.sum() < min_cells_per_group or ntc_mask_pred.sum() < min_cells_per_group:\n",
    "            df_pred = pd.DataFrame(index=gene_names, data={\n",
    "                'pval': np.ones(G),\n",
    "                'pval_adj': np.ones(G),\n",
    "                'significant': np.zeros(G, dtype=bool),\n",
    "                'logfc': np.zeros(G),\n",
    "                'mean_group': np.zeros(G),\n",
    "                'mean_ntc': np.zeros(G)\n",
    "            })\n",
    "        else:\n",
    "            df_pred = _wilcoxon_de_genes(pred_adata, g_mask_pred, ntc_mask_pred, gene_names, fdr=fdr, sample_n=sample_n)\n",
    "\n",
    "        G_pred = set(df_pred.index[df_pred['significant']].tolist())\n",
    "\n",
    "        # 若 |G_pred| <= |G_true|，直接取交集\n",
    "        if len(G_true) == 0:\n",
    "            # 真实没有 DE 基因：规范上可跳过（用 NaN），或若两者都为空算 1.0，我们选择跳过（不计入均值）\n",
    "            DES_list.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        if len(G_pred) <= len(G_true):\n",
    "            inter = len(G_pred & G_true)\n",
    "            DES_k = inter / len(G_true)\n",
    "            DES_list.append(DES_k)\n",
    "        else:\n",
    "            # 预测过大，按 logFC 的绝对值挑 top |G_true|（使用预测的 logFC）\n",
    "            df_pred_sorted = df_pred.reindex(gene_names).copy()\n",
    "            # 以绝对 logFC 排序，取 top\n",
    "            df_pred_sorted['abs_logfc'] = np.abs(df_pred_sorted['logfc'])\n",
    "            topk = int(len(G_true))\n",
    "            top_genes = set(df_pred_sorted.sort_values('abs_logfc', ascending=False).head(topk).index.tolist())\n",
    "            inter = len(top_genes & G_true)\n",
    "            DES_k = inter / len(G_true)\n",
    "            DES_list.append(DES_k)\n",
    "\n",
    "    DES_array = np.array(DES_list, dtype=float)\n",
    "    DES_mean = np.nanmean(DES_array)  # 忽略 NaN\n",
    "\n",
    "    # --- 2) 计算 PDS ---\n",
    "    # 计算所有 perturbation 的 pseudobulk mean expression (true & pred), 包括 ntc\n",
    "    # 以 group 名字为 key\n",
    "    true_pseudobulk = {}\n",
    "    pred_pseudobulk = {}\n",
    "    for grp in list(true_groups.unique()):\n",
    "        mask = (true_groups == grp).values\n",
    "        true_pseudobulk[grp] = _get_matrix_row_mean(true_adata, mask)\n",
    "    for grp in list(pred_groups.unique()):\n",
    "        mask = (pred_groups == grp).values\n",
    "        pred_pseudobulk[grp] = _get_matrix_row_mean(pred_adata, mask)\n",
    "\n",
    "    # 若某些 perturbation 在 pred 中缺失，则用 zeros 或跳过（这里我们尽力用 zeros 并发警告）\n",
    "    for p in perturbations:\n",
    "        if p not in pred_pseudobulk:\n",
    "            warnings.warn(f\"Perturbation {p} missing in predicted data; using zeros vector.\")\n",
    "            pred_pseudobulk[p] = np.zeros(G)\n",
    "        if p not in true_pseudobulk:\n",
    "            warnings.warn(f\"Perturbation {p} missing in true data; using zeros vector.\")\n",
    "            true_pseudobulk[p] = np.zeros(G)\n",
    "\n",
    "    if control_label not in true_pseudobulk or control_label not in pred_pseudobulk:\n",
    "        raise ValueError(\"Control label not present in pseudobulk results for true or pred.\")\n",
    "\n",
    "    # 计算 delta = x_k - x_ntc\n",
    "    true_delta = {p: true_pseudobulk[p] - true_pseudobulk[control_label] for p in perturbations}\n",
    "    pred_delta = {p: pred_pseudobulk[p] - pred_pseudobulk[control_label] for p in perturbations}\n",
    "\n",
    "    PDS_vals = []\n",
    "    for p in perturbations:\n",
    "        # 计算 pred_delta[p] 与所有 true_delta[q] 的 L1 距离\n",
    "        dists = np.array([np.sum(np.abs(pred_delta[p] - true_delta[q])) for q in perturbations])\n",
    "        # 找到真实配对 q==p 的距离的排名（1 是最小）\n",
    "        # rank = 1 + number of distances strictly smaller than d_pt\n",
    "        d_pt = np.sum(np.abs(pred_delta[p] - true_delta[p]))\n",
    "        rank = int((dists < d_pt).sum()) + 1\n",
    "        # PDS_p = 1 - (rank-1)/N\n",
    "        PDS_p = 1.0 - (rank - 1) / float(N)\n",
    "        PDS_vals.append(PDS_p)\n",
    "\n",
    "    PDS_array = np.array(PDS_vals, dtype=float)\n",
    "    PDS_mean = PDS_array.mean()\n",
    "\n",
    "    # --- 3) 计算 MAE ---\n",
    "    MAE_vals = []\n",
    "    for p in perturbations:\n",
    "        y_true = true_pseudobulk[p]\n",
    "        y_pred = pred_pseudobulk.get(p, np.zeros(G))\n",
    "        mae_k = np.mean(np.abs(y_pred - y_true))\n",
    "        MAE_vals.append(mae_k)\n",
    "    MAE_array = np.array(MAE_vals, dtype=float)\n",
    "    MAE_mean = MAE_array.mean()\n",
    "\n",
    "    # --- baseline handling: 若提供 baseline_adata，则计算 baseline 的三项；否则用 baseline_scores dict ---\n",
    "    if baseline_scores is None and baseline_adata is not None:\n",
    "        # 递归调用 compute_vcc_scores，将 baseline_scores 作为返回值（但为防循环，这里传 None 且 baseline_adata=None）\n",
    "        baseline_res = compute_vcc_scores(true_adata, baseline_adata,\n",
    "                                          baseline_scores={'DES':0.0,'PDS':0.0,'MAE':1.0},\n",
    "                                          groupby=groupby, control_label=control_label,\n",
    "                                          fdr=fdr, sample_n=sample_n, min_cells_per_group=min_cells_per_group)\n",
    "        baseline_scores = {'DES': baseline_res['DES'],\n",
    "                           'PDS': baseline_res['PDS'],\n",
    "                           'MAE': baseline_res['MAE']}\n",
    "    if baseline_scores is None:\n",
    "        raise ValueError(\"You must provide either baseline_scores or baseline_adata to compute scaled scores.\")\n",
    "\n",
    "    DES_baseline = float(baseline_scores.get('DES', 0.0))\n",
    "    PDS_baseline = float(baseline_scores.get('PDS', 0.0))\n",
    "    MAE_baseline = float(baseline_scores.get('MAE', 1.0))\n",
    "\n",
    "    # --- 计算 scaled scores ---\n",
    "    # 对 DES 和 PDS 在 [0,1] 上缩放；对 MAE 用相对减小量\n",
    "    # 防止分母为 0\n",
    "    def safe_scale_score(pred, base):\n",
    "        if base >= 1.0 - 1e-12:\n",
    "            return 0.0\n",
    "        return (pred - base) / (1.0 - base)\n",
    "\n",
    "    DES_scaled = safe_scale_score(DES_mean, DES_baseline)\n",
    "    PDS_scaled = safe_scale_score(PDS_mean, PDS_baseline)\n",
    "    # MAE_scaled = (MAE_baseline - MAE_pred) / MAE_baseline\n",
    "    if MAE_baseline == 0:\n",
    "        MAE_scaled = 0.0\n",
    "    else:\n",
    "        MAE_scaled = (MAE_baseline - MAE_mean) / MAE_baseline\n",
    "\n",
    "    overall_score = (DES_scaled + PDS_scaled + MAE_scaled) / 3.0 * 100.0\n",
    "\n",
    "    result = {\n",
    "        'DES': float(DES_mean),\n",
    "        'PDS': float(PDS_mean),\n",
    "        'MAE': float(MAE_mean),\n",
    "        'DES_scaled': float(DES_scaled),\n",
    "        'PDS_scaled': float(PDS_scaled),\n",
    "        'MAE_scaled': float(MAE_scaled),\n",
    "        'overall_score_percent': float(overall_score),\n",
    "        # 额外返回每 perturbation 的详细值（可选查看）\n",
    "        'DES_per_perturbation': pd.Series(DES_array, index=perturbations),\n",
    "        'PDS_per_perturbation': pd.Series(PDS_array, index=perturbations),\n",
    "        'MAE_per_perturbation': pd.Series(MAE_array, index=perturbations),\n",
    "        'perturbations': perturbations\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6df9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AnnData files...\n",
      "Computing scores (this may take time depending on dataset size)...\n",
      "\n",
      "===== Summary =====\n",
      "DES (mean): 1.000000\n",
      "PDS (mean): 1.000000\n",
      "MAE (mean): 0.000000\n",
      "DES_scaled: 1.000000\n",
      "PDS_scaled: 1.000000\n",
      "MAE_scaled: 1.000000\n",
      "Overall score (percent): 100.0000%\n",
      "\n",
      "Saved evaluation_results.json and evaluation_per_perturbation.csv\n"
     ]
    }
   ],
   "source": [
    "true_file = \"../small_set.h5ad\"\n",
    "pred_file = \"../small_set.h5ad\"\n",
    "\n",
    "print(\"Loading AnnData files...\")\n",
    "true_adata = ad.read_h5ad(true_file)\n",
    "pred_adata = ad.read_h5ad(pred_file)\n",
    "\n",
    "baseline_scores = {'DES': 0.106, 'PDS': 0.514, 'MAE': 0.027}\n",
    "\n",
    "print(\"Computing scores (this may take time depending on dataset size)...\")\n",
    "scores = compute_vcc_scores(true_adata, pred_adata,\n",
    "                            baseline_scores=baseline_scores,\n",
    "                            groupby='target_gene',\n",
    "                            control_label='non-targeting',\n",
    "                            fdr=0.05,\n",
    "                            sample_n=1000,\n",
    "                            min_cells_per_group=3)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n===== Summary =====\")\n",
    "print(f\"DES (mean): {scores['DES']:.6f}\")\n",
    "print(f\"PDS (mean): {scores['PDS']:.6f}\")\n",
    "print(f\"MAE (mean): {scores['MAE']:.6f}\")\n",
    "print(f\"DES_scaled: {scores['DES_scaled']:.6f}\")\n",
    "print(f\"PDS_scaled: {scores['PDS_scaled']:.6f}\")\n",
    "print(f\"MAE_scaled: {scores['MAE_scaled']:.6f}\")\n",
    "print(f\"Overall score (percent): {scores['overall_score_percent']:.4f}%\")\n",
    "\n",
    "# Save overall and per-perturbation results\n",
    "out_json = {\n",
    "    'DES': scores['DES'],\n",
    "    'PDS': scores['PDS'],\n",
    "    'MAE': scores['MAE'],\n",
    "    'DES_scaled': scores['DES_scaled'],\n",
    "    'PDS_scaled': scores['PDS_scaled'],\n",
    "    'MAE_scaled': scores['MAE_scaled'],\n",
    "    'overall_score_percent': scores['overall_score_percent']\n",
    "}\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(out_json, f, indent=2)\n",
    "\n",
    "# save per-perturbation CSVs\n",
    "df_des = scores['DES_per_perturbation'].rename(\"DES\").to_frame()\n",
    "df_pds = scores['PDS_per_perturbation'].rename(\"PDS\").to_frame()\n",
    "df_mae = scores['MAE_per_perturbation'].rename(\"MAE\").to_frame()\n",
    "df_all = pd.concat([df_des, df_pds, df_mae], axis=1)\n",
    "df_all.to_csv('evaluation_per_perturbation.csv')\n",
    "\n",
    "print(\"\\nSaved evaluation_results.json and evaluation_per_perturbation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d00d9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AnnData files...\n",
      "Computing scores (this may take time depending on dataset size)...\n",
      "\n",
      "===== Summary =====\n",
      "DES (mean): 0.042667\n",
      "PDS (mean): 0.516667\n",
      "MAE (mean): 7.005270\n",
      "DES_scaled: -0.070842\n",
      "PDS_scaled: 0.005487\n",
      "MAE_scaled: -258.454442\n",
      "Overall score (percent): -8617.3266%\n",
      "\n",
      "Saved evaluation_results.json and evaluation_per_perturbation.csv\n"
     ]
    }
   ],
   "source": [
    "true_file = \"../test_set_1119.h5ad\"\n",
    "pred_file = \"example.h5ad\"\n",
    "\n",
    "print(\"Loading AnnData files...\")\n",
    "true_adata = ad.read_h5ad(true_file)\n",
    "pred_adata = ad.read_h5ad(pred_file)\n",
    "\n",
    "baseline_scores = {'DES': 0.106, 'PDS': 0.514, 'MAE': 0.027}\n",
    "\n",
    "print(\"Computing scores (this may take time depending on dataset size)...\")\n",
    "scores = compute_vcc_scores(true_adata, pred_adata,\n",
    "                            baseline_scores=baseline_scores,\n",
    "                            groupby='target_gene',\n",
    "                            control_label='non-targeting',\n",
    "                            fdr=0.05,\n",
    "                            sample_n=1000,\n",
    "                            min_cells_per_group=3)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n===== Summary =====\")\n",
    "print(f\"DES (mean): {scores['DES']:.6f}\")\n",
    "print(f\"PDS (mean): {scores['PDS']:.6f}\")\n",
    "print(f\"MAE (mean): {scores['MAE']:.6f}\")\n",
    "print(f\"DES_scaled: {scores['DES_scaled']:.6f}\")\n",
    "print(f\"PDS_scaled: {scores['PDS_scaled']:.6f}\")\n",
    "print(f\"MAE_scaled: {scores['MAE_scaled']:.6f}\")\n",
    "print(f\"Overall score (percent): {scores['overall_score_percent']:.4f}%\")\n",
    "\n",
    "# Save overall and per-perturbation results\n",
    "out_json = {\n",
    "    'DES': scores['DES'],\n",
    "    'PDS': scores['PDS'],\n",
    "    'MAE': scores['MAE'],\n",
    "    'DES_scaled': scores['DES_scaled'],\n",
    "    'PDS_scaled': scores['PDS_scaled'],\n",
    "    'MAE_scaled': scores['MAE_scaled'],\n",
    "    'overall_score_percent': scores['overall_score_percent']\n",
    "}\n",
    "with open('eval_outcome/evaluation_results.json', 'w') as f:\n",
    "    json.dump(out_json, f, indent=2)\n",
    "\n",
    "# save per-perturbation CSVs\n",
    "df_des = scores['DES_per_perturbation'].rename(\"DES\").to_frame()\n",
    "df_pds = scores['PDS_per_perturbation'].rename(\"PDS\").to_frame()\n",
    "df_mae = scores['MAE_per_perturbation'].rename(\"MAE\").to_frame()\n",
    "df_all = pd.concat([df_des, df_pds, df_mae], axis=1)\n",
    "df_all.to_csv('eval_outcome/valuation_per_perturbation.csv')\n",
    "\n",
    "print(\"\\nSaved evaluation_results.json and evaluation_per_perturbation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "state1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
